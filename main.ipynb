{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Imports & Setup`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### External Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import json\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Config Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"full\", \"first_half\", \"second_half\"\n",
    "row_range = \"full\"\n",
    "\n",
    "# picking only the top k related paragraphs\n",
    "find_top_k = 4\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = device.type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_into_paragraphs(text, chunk_size=3, merge_headings=True):\n",
    "    \"\"\"Split policy text into N-sentence chunks.\"\"\"\n",
    "\n",
    "    # Fix encoding and glued terms (e.g. BenchmarkRate → Benchmark Rate)\n",
    "    text = text.replace(\"�\", \" \").replace(\"•\", \"*\")\n",
    "    text = re.sub(r'(?<=[a-z])(?=[A-Z])', ' ', text)        # aB → a B\n",
    "    text = re.sub(r'(?<=\\d)(?=[A-Z])', ' ', text)           # 25Years → 25 Years\n",
    "    text = re.sub(r'(?<=[a-zA-Z])(?=\\d)', ' ', text)        # abc123 → abc 123\n",
    "    text = re.sub(r'(?<=[a-z])(?=[A-Z][a-z])', '. ', text)  # add inferred periods\n",
    "\n",
    "    # Normalize spacing\n",
    "    text = re.sub(r'\\n{2,}', '\\n', text)      # collapse double line breaks\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # remove excess whitespace\n",
    "\n",
    "    # Split at likely section headings\n",
    "    sections = re.split(r'\\n(?=[A-Z][^\\n]{3,60}\\n)', text)\n",
    "\n",
    "    tokenizer = PunktSentenceTokenizer()\n",
    "    chunks = []\n",
    "\n",
    "    for section in sections:\n",
    "        section = section.strip()\n",
    "        if not section:\n",
    "            continue\n",
    "\n",
    "        # Convert bullet-style lines into full sentences\n",
    "        section = re.sub(r\"\\n\\s*\\*\\s*\", \". \", section)\n",
    "        section = re.sub(r\"\\*\\s*\", \"\", section)\n",
    "\n",
    "        # Break before heading-like phrases\n",
    "        section = re.sub(r'(?<=\\. )([A-Z][^\\n]{3,60})(?= )', r'\\n\\1', section)\n",
    "\n",
    "        sentences = tokenizer.tokenize(section)\n",
    "\n",
    "        # Attach short heading-only lines to previous chunk\n",
    "        if merge_headings and len(sentences) <= 1 and chunks:\n",
    "            chunks[-1] += \" \" + section\n",
    "            continue\n",
    "\n",
    "        # Group into N-sentence chunks\n",
    "        for i in range(0, len(sentences), chunk_size):\n",
    "            chunk = \" \".join(sentences[i:i + chunk_size])\n",
    "            chunks.append(chunk.strip())\n",
    "\n",
    "    return chunks\n",
    "def get_embedding(text, model=\"nomic-embed-text\"):\n",
    "    if device == \"cuda\":\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"intfloat/e5-base\")\n",
    "        model = AutoModel.from_pretrained(\"intfloat/e5-base\").to(\"cuda\")\n",
    "        \n",
    "        if isinstance(text, str):\n",
    "            text = [text]\n",
    "        \n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        return embeddings.cpu().detach().numpy().tolist()[0]\n",
    "    else:\n",
    "        url = \"http://localhost:11434/api/embed\"\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"input\": text\n",
    "        }\n",
    "        response = requests.post(url, json=payload)\n",
    "        return response.json()[\"embeddings\"][0]\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    vec1, vec2 = np.array(vec1), np.array(vec2)\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "def generate_prompt(policy_of_interest, policy_of_interest_embedding, paragraph_embeddings):\n",
    "    most_related_paragraphs = []\n",
    "\n",
    "    for index in range(len(paragraph_embeddings)):\n",
    "        score = cosine_similarity(policy_of_interest_embedding, paragraph_embeddings[index])\n",
    "        most_related_paragraphs.append((paragraphs[index], score))\n",
    "        \n",
    "    most_related_paragraphs.sort(key=lambda x: x[1], reverse=True)\n",
    "    most_related_paragraphs = most_related_paragraphs[:find_top_k]\n",
    "    combined_paragraphs = \"\\n\".join([p[0] for p in most_related_paragraphs])\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "        You are to extract form-related policies from the attached 'Policies_Meridian.docx'. Focus on identifying both **explicit mentions** and **indirect references** (e.g., policies embedded in documentation procedures or described without using exact policy names).\n",
    "\n",
    "        The 'Policies_Meridian.docx' file content is provided below:\n",
    "\n",
    "        {combined_paragraphs}\n",
    "\n",
    "        The policy of interest is \"{policy_of_interest}\" with Search Terms Synonyms \"{search_terms_synonyms}\" for targeting the location of that policy in the doc. \n",
    "        Remember to extract not just explicit mentions but also policies that are implied or embedded in procedures. \n",
    "        This is a very important policy detail document, and many people's decisions will depend on the accuracy of your response.\n",
    "\n",
    "        Output the result in the following format:\n",
    "\n",
    "        {{\n",
    "            \"Y/N/M\": \"Y\",\n",
    "            \"POLICY DETAILS\": \"Meridian requires signed consent from all applicants prior to underwriting, but does not provide consent forms to brokers.\"\n",
    "        }}\n",
    "\n",
    "        For each item, say:\n",
    "        - \"Y\" if it is clearly mentioned, which means the policy is explicitly stated in the text without any ambiguity.\n",
    "        - \"M\" if it is mentioned indirectly, which means the policy is implied or embedded in procedures, but not explicitly stated.\n",
    "        - \"N\" if not found, which means the policy is not mentioned at all.\n",
    "\n",
    "        - \"POLICY DETAILS\" should contain the specific details of the policy as mentioned in the text. If the policy is not mentioned, leave it blank.\n",
    "    \"\"\".strip()\n",
    "\n",
    "    return prompt\n",
    "def request_extracted_policy_detail_from_ollama(prompt, policy_of_interest):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    data = {\n",
    "        \"model\": \"deepseek-r1:8b\",\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"format\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"Y/N/M\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"Y\", \"M\", \"N\"]\n",
    "                },\n",
    "                \"POLICY DETAILS\": {\n",
    "                    \"type\": \"string\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"Y/N/M\", \"POLICY DETAILS\"]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    response = response.json()\n",
    "    response_data = response['response']\n",
    "    response_data = json.loads(response_data)\n",
    "    response_data[\"POLICY NAME\"] = policy_of_interest\n",
    "    response_data = json.dumps(response_data)\n",
    "    return response_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Load & Preprocess`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load DOCX and embedding them into vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/preprocessed_data/policies_meridian_plaintext.txt\", \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "    policies_meridian_plaintext = \"\\n\".join(f.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = split_text_into_paragraphs(policies_meridian_plaintext)\n",
    "paragraph_embeddings = []\n",
    "for p in paragraphs:\n",
    "   paragraph_embeddings.append(get_embedding(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Policies and embedding them into vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k:\\GIT\\mentor_group_side_project\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/preprocessed_data/policies_to_update.csv\")\n",
    "df = df.drop(index=[0,1])\n",
    "df = df.drop(columns=[\"Y/N/M\", \"POLICY DETAILS\", \"Prompt\"])\n",
    "if row_range == \"full\":\n",
    "    df = df.iloc[0:len(df)]\n",
    "elif row_range == \"first_half\":\n",
    "    df = df.iloc[0:len(df)//2]\n",
    "elif row_range == \"second_half\":\n",
    "    df = df.iloc[len(df)//2:len(df)]\n",
    "else:\n",
    "    df = df.iloc[0:len(df)]\n",
    " \n",
    "policy_of_interests = df[\"POLICY NAME\"].tolist()\n",
    "search_terms_synonyms = df[\"Search Terms Synonyms\"].tolist()\n",
    "policy_of_interest_embeddings = []\n",
    "\n",
    "for p in policy_of_interests:\n",
    "    policy_of_interest_embeddings.append(get_embedding(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Generate Table`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loop through each policy of interest and generate the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1/248: BFS (CMHC Program) → Y/M\n",
      "Processed 2/248: BFS ALT-A → Y/M\n",
      "Processed 3/248: BFS Stated Income (Bank Statements) → Y/M\n",
      "Processed 4/248: BFS Stated Income (Conventional) → Y/M\n",
      "Processed 5/248: BFS Stated Income (Sagen & CG Program) → Y/M\n",
      "Processed 6/248: Cash Back Mortgages → Y/M\n",
      "Processed 7/248: Collateral Switch/Transfer → Y/M\n",
      "Processed 8/248: Construction → Y/M\n",
      "Processed 9/248: Cottage/Recreational Properties → Y/M\n",
      "Processed 10/248: Equity Program → Y/M\n",
      "Processed 11/248: Flex Down Payments (ie; Credit Cards) → Y/M\n",
      "Processed 12/248: Foreign Borrowers (Non-Residents) → Y/M\n",
      "Processed 13/248: Limited Feature Mortgages → Y/M\n",
      "Processed 14/248: Medical Professionals Program → Y/M\n",
      "Processed 15/248: Mortgage & HELOC Combinations → Y/M\n",
      "Processed 16/248: Net Worth Program → Y/M\n",
      "Processed 17/248: New to Canada → Y/M\n",
      "Processed 18/248: New to Canada (Rental) → Y/M\n",
      "Processed 19/248: No-fee Alt Lender → Y/M\n",
      "Processed 20/248: Non-Permanent/Temporary Residents → Y/M\n",
      "Processed 21/248: Open Mortgages → Y/M\n",
      "Processed 22/248: Pre-Approval Programs → Y/M\n",
      "Processed 23/248: Pre-Approval Programs With No Premium → Y/M\n",
      "Processed 24/248: Professionals → Y/M\n",
      "Processed 25/248: Purchase Plus Improvements → Y/M\n",
      "Processed 26/248: Purchase Plus Improvements Uninsurable → Y/M\n",
      "Processed 27/248: Raw/Vacant Land Financing → Y/M\n",
      "Processed 28/248: Refinance → Y/M\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "prompts = []\n",
    "parsed_results = []\n",
    "n_count = 0\n",
    "\n",
    "for i, policy_of_interest in enumerate(policy_of_interests):\n",
    "    policy_of_interest_embedding = policy_of_interest_embeddings[i]\n",
    "    prompt = generate_prompt(policy_of_interest, policy_of_interest_embedding, paragraph_embeddings)\n",
    "    prompts.append(prompt)\n",
    "\n",
    "for i, policy_of_interest in enumerate(policy_of_interests):\n",
    "    extracted_policy_detail = request_extracted_policy_detail_from_ollama(prompts[i], policy_of_interest)\n",
    "    results.append(extracted_policy_detail)\n",
    "\n",
    "    try:\n",
    "        detail_dict = json.loads(extracted_policy_detail)\n",
    "        parsed_results.append(detail_dict)\n",
    "\n",
    "        if detail_dict.get(\"Y/N/M\", \"\").strip() == \"N\":\n",
    "            n_count += 1\n",
    "            status = \"N\"\n",
    "        else:\n",
    "            status = \"Y/M\"\n",
    "\n",
    "    except Exception as e:\n",
    "        parsed_results.append({\n",
    "            \"POLICY NAME\": policy_of_interest,\n",
    "            \"Y/N/M\": \"ERROR\",\n",
    "            \"POLICY DETAILS\": f\"Failed to parse: {str(e)}\"\n",
    "        })\n",
    "        status = \"Parse Error\"\n",
    "\n",
    "    print(f\"Processed {i+1}/{len(policy_of_interests)}: {policy_of_interest} → {status}\")\n",
    "\n",
    "print(f\"\\n Total 'N' results: {n_count} / {len(policy_of_interests)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "# df_results.to_csv(\"../../data/preprocessed_data/extracted_policy_details.csv\", index=False)\n",
    "# df_results.to_csv(\"../../data/preprocessed_data/extracted_policy_details_updated.csv\", index=False)\n",
    "results_df.to_csv(\"./extracted_policy_details_updated(first_half).csv\", index=False)\n",
    "# df_results.to_csv(\"../../data/preprocessed_data/extracted_policy_details_updated(second_half).csv\", index=False)\n",
    "# df_results.to_csv(\"../../data/preprocessed_data/extracted_policy_details_updated(full).csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
