{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Imports & Setup`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### External Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import json\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### >>> Config Variables <<<"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"full\", \"first_half\", \"second_half\", \"random_10\"\n",
    "row_range = \"full\"\n",
    "\n",
    "# { find_top_k } number of paragraphs that have the highest similarity score with the policy and search terms will be used to generate the prompt.\n",
    "find_top_k = 4\n",
    "\n",
    "# score of similarity = { policy_weight } * cosine_similarity between policy and paragraph + { 1 - policy_weight } * cosine_similarity between policy and search terms\n",
    "policy_weight = 0.6\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = device.type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_progress_bar(iteration, total, prefix='', length=40, start_time=None, line_width=256):\n",
    "    elapsed = time.time() - start_time if start_time else 0\n",
    "    avg_time = elapsed / iteration if iteration > 0 else 0\n",
    "    eta = avg_time * (total - iteration)\n",
    "\n",
    "    percent = f\"{100 * (iteration / float(total)):.1f}\"\n",
    "    filled_length = int(length * iteration // total)\n",
    "    bar_color = '\\033[32m'\n",
    "    bar = bar_color + '█' * filled_length + '-' * (length - filled_length) + '\\033[0m'\n",
    "\n",
    "    eta_min = int(eta // 60)\n",
    "    eta_sec = int(eta % 60)\n",
    "\n",
    "    line = f\"|{bar}| {percent}% Complete | ETA: {eta_min}m {eta_sec}s | {prefix}\"\n",
    "    padded_line = line.ljust(line_width)\n",
    "\n",
    "    sys.stdout.write('\\r' + padded_line)\n",
    "    sys.stdout.flush()\n",
    "def split_text_into_paragraphs(text, chunk_size=3, merge_headings=True):\n",
    "    \"\"\"Split policy text into N-sentence chunks.\"\"\"\n",
    "\n",
    "    # Fix encoding and glued terms (e.g. BenchmarkRate → Benchmark Rate)\n",
    "    text = text.replace(\"�\", \" \").replace(\"•\", \"*\")\n",
    "    text = re.sub(r'(?<=[a-z])(?=[A-Z])', ' ', text)        # aB → a B\n",
    "    text = re.sub(r'(?<=\\d)(?=[A-Z])', ' ', text)           # 25Years → 25 Years\n",
    "    text = re.sub(r'(?<=[a-zA-Z])(?=\\d)', ' ', text)        # abc123 → abc 123\n",
    "    text = re.sub(r'(?<=[a-z])(?=[A-Z][a-z])', '. ', text)  # add inferred periods\n",
    "\n",
    "    # Normalize spacing\n",
    "    text = re.sub(r'\\n{2,}', '\\n', text)      # collapse double line breaks\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # remove excess whitespace\n",
    "\n",
    "    # Split at likely section headings\n",
    "    sections = re.split(r'\\n(?=[A-Z][^\\n]{3,60}\\n)', text)\n",
    "\n",
    "    tokenizer = PunktSentenceTokenizer()\n",
    "    chunks = []\n",
    "\n",
    "    for section in sections:\n",
    "        section = section.strip()\n",
    "        if not section:\n",
    "            continue\n",
    "\n",
    "        # Convert bullet-style lines into full sentences\n",
    "        section = re.sub(r\"\\n\\s*\\*\\s*\", \". \", section)\n",
    "        section = re.sub(r\"\\*\\s*\", \"\", section)\n",
    "\n",
    "        # Break before heading-like phrases\n",
    "        section = re.sub(r'(?<=\\. )([A-Z][^\\n]{3,60})(?= )', r'\\n\\1', section)\n",
    "\n",
    "        sentences = tokenizer.tokenize(section)\n",
    "\n",
    "        # Attach short heading-only lines to previous chunk\n",
    "        if merge_headings and len(sentences) <= 1 and chunks:\n",
    "            chunks[-1] += \" \" + section\n",
    "            continue\n",
    "\n",
    "        # Group into N-sentence chunks\n",
    "        for i in range(0, len(sentences), chunk_size):\n",
    "            chunk = \" \".join(sentences[i:i + chunk_size])\n",
    "            chunks.append(chunk.strip())\n",
    "\n",
    "    return chunks\n",
    "def get_embedding(text, model=\"nomic-embed-text\"):\n",
    "    if device == \"cuda\":\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"intfloat/e5-base\")\n",
    "        model = AutoModel.from_pretrained(\"intfloat/e5-base\").to(\"cuda\")\n",
    "        \n",
    "        if isinstance(text, str):\n",
    "            text = [text]\n",
    "        \n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        return embeddings.cpu().detach().numpy().tolist()[0]\n",
    "    else:\n",
    "        url = \"http://localhost:11434/api/embed\"\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"input\": text\n",
    "        }\n",
    "        response = requests.post(url, json=payload)\n",
    "        return response.json()[\"embeddings\"][0]\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    vec1, vec2 = np.array(vec1), np.array(vec2)\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "def generate_prompt(policy_of_interest, policy_of_interest_embedding, paragraph_embeddings, search_terms_synonyms_embedding):\n",
    "    most_related_paragraphs = []\n",
    "\n",
    "    for index in range(len(paragraph_embeddings)):\n",
    "        score = policy_weight * cosine_similarity(policy_of_interest_embedding, paragraph_embeddings[index]) + (1 - policy_weight) * cosine_similarity(search_terms_synonyms_embedding, paragraph_embeddings[index])\n",
    "        most_related_paragraphs.append((paragraphs[index], score))\n",
    "        \n",
    "    most_related_paragraphs.sort(key=lambda x: x[1], reverse=True)\n",
    "    most_related_paragraphs = most_related_paragraphs[:find_top_k]\n",
    "    combined_paragraphs = \"\\n\".join([p[0] for p in most_related_paragraphs])\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "        You are an expert in policy analysis. Your task is to extract **form-related policies** from the document titled *Policies_Meridian.docx*. These policies may be:\n",
    "\n",
    "        1. **Explicitly stated** - directly mentioned in the text.\n",
    "        2. **Implicitly referenced** - embedded within procedures, documentation processes, or described indirectly without using the exact policy names.\n",
    "\n",
    "        ---\n",
    "        \n",
    "        ### Document Content:\n",
    "        Below is the full content of *Policies_Meridian.docx*:\n",
    "        \n",
    "        {combined_paragraphs}\n",
    "        \n",
    "        ---\n",
    "\n",
    "        ### Target Policy:\n",
    "        - Policy of Interest: **\"{policy_of_interest}\"**\n",
    "        - Search Term Synonyms: **\"{search_terms_synonyms}\"**\n",
    "        \n",
    "        Use these keywords and any related concepts to locate relevant policies. \n",
    "        Remember to extract not just explicit mentions but also policies that are implied or embedded in procedures.\n",
    "        This document is used for important decision-making. Ensure no relevant information is overlooked, whether it's directly stated or subtly implied.\n",
    "        \n",
    "        ---\n",
    "        \n",
    "        ### Output Instructions:\n",
    "        \n",
    "        For **each policy instance** found, provide the following:\n",
    "\n",
    "        1. **Status**:\n",
    "        - `\"Y\"` - Clearly mentioned (explicitly and unambiguously stated).\n",
    "        - `\"M\"` - Mentioned indirectly (implied, inferred, or part of a procedure).\n",
    "        - `\"N\"` - Not found (no relevant mention in the document).\n",
    "\n",
    "        2. **POLICY DETAILS**:\n",
    "        - Include the exact wording, numbers, or phrases that support your judgment.\n",
    "        - If no policy is found, leave this field empty, returning `\"\"`.\n",
    "        - If the Response is `\"N\"`, the field should be empty, returning `\"\"`.\n",
    "\n",
    "        ---\n",
    "    \"\"\".strip()\n",
    "\n",
    "    return prompt\n",
    "def request_extracted_policy_detail_from_ollama(prompt, policy_of_interest):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    data = {\n",
    "        \"model\": \"deepseek-r1:8b\",\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"format\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"Status\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"Y\", \"M\", \"N\"]\n",
    "                },\n",
    "                \"POLICY DETAILS\": {\n",
    "                    \"type\": \"string\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"Status\", \"POLICY DETAILS\"]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    response = response.json()\n",
    "    response_data = response['response']\n",
    "    response_data = json.loads(response_data)\n",
    "    response_data[\"POLICY NAME\"] = policy_of_interest\n",
    "    response_data = json.dumps(response_data)\n",
    "    return response_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Load & Preprocess`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load DOCX and embedding them into vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/preprocessed_data/policies_meridian_plaintext.txt\", \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "    policies_meridian_plaintext = \"\\n\".join(f.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|\u001b[34m████████████████████████████████████████\u001b[0m| 100.0% Complete | ETA: 0m 0s | Embeddings                                   "
     ]
    }
   ],
   "source": [
    "paragraphs = split_text_into_paragraphs(policies_meridian_plaintext)\n",
    "paragraph_embeddings = []\n",
    "\n",
    "start_time = time.time()\n",
    "for p in paragraphs:\n",
    "   paragraph_embeddings.append(get_embedding(p))\n",
    "   print_progress_bar(len(paragraph_embeddings), len(paragraphs), prefix=\"Embeddings\", start_time=start_time, line_width=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Policies & Search Terms Synonyms and embedding them into vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k:\\GIT\\mentor_group_side_project\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|\u001b[34m████████████████████████████████████████\u001b[0m| 100.0% Complete | ETA: 0m 0s | Embeddings                                   "
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/preprocessed_data/policies_to_update.csv\")\n",
    "df = df.drop(index=[0,1])\n",
    "df = df.drop(columns=[\"Y/N/M\", \"POLICY DETAILS\", \"Prompt\"])\n",
    "if row_range == \"full\":\n",
    "    df = df.iloc[0:len(df)]\n",
    "elif row_range == \"first_half\":\n",
    "    df = df.iloc[0:len(df)//2]\n",
    "elif row_range == \"second_half\":\n",
    "    df = df.iloc[len(df)//2:len(df)]\n",
    "elif row_range == \"random_10\":\n",
    "    df = df.sample(n=10, random_state=42)\n",
    "else:\n",
    "    df = df.iloc[0:len(df)]\n",
    " \n",
    "policy_of_interests = df[\"POLICY NAME\"].tolist()\n",
    "search_terms_synonyms = df[\"Search Terms Synonyms\"].tolist()\n",
    "policy_of_interest_embeddings = []\n",
    "search_terms_synonyms_embeddings = []\n",
    "\n",
    "start_time = time.time()\n",
    "for p in policy_of_interests:\n",
    "    policy_of_interest_embeddings.append(get_embedding(p))\n",
    "    print_progress_bar(len(policy_of_interest_embeddings), len(policy_of_interests), prefix=\"Embeddings\", start_time=start_time, line_width=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|\u001b[34m████████████████████████████████████████\u001b[0m| 100.0% Complete | ETA: 0m 0s | Embeddings                                   "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for s in search_terms_synonyms:\n",
    "    search_terms_synonyms_embeddings.append(get_embedding(str(s)))\n",
    "    print_progress_bar(len(search_terms_synonyms_embeddings), len(search_terms_synonyms), prefix=\"Embeddings\", start_time=start_time, line_width=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Generate Table`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loop through each policy of interest and generate the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|\u001b[34m████████████████████████████████████████\u001b[0m| 100.0% Complete | ETA: 0m 0s | 0 N's | NICHES → Y/M                                                                                                                                                         "
     ]
    }
   ],
   "source": [
    "results = []\n",
    "prompts = []\n",
    "parsed_results = []\n",
    "n_count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for i, policy_of_interest in enumerate(policy_of_interests):\n",
    "    policy_of_interest_embedding = policy_of_interest_embeddings[i]\n",
    "    search_terms_synonyms_embedding = search_terms_synonyms_embeddings[i]\n",
    "    prompt = generate_prompt(policy_of_interest, policy_of_interest_embedding, paragraph_embeddings, search_terms_synonyms_embedding)\n",
    "    prompts.append(prompt)\n",
    "\n",
    "for i, policy_of_interest in enumerate(policy_of_interests):\n",
    "    extracted_policy_detail = request_extracted_policy_detail_from_ollama(prompts[i], policy_of_interest)\n",
    "    results.append(extracted_policy_detail)\n",
    "\n",
    "    try:\n",
    "        detail_dict = json.loads(extracted_policy_detail)\n",
    "        parsed_results.append(detail_dict)\n",
    "        \n",
    "        if detail_dict.get(\"Status\", \"\").strip() == \"N\":\n",
    "            n_count += 1\n",
    "            status = \"N\"\n",
    "        else:\n",
    "            status = \"Y/M\"\n",
    "\n",
    "    except Exception as e:\n",
    "        parsed_results.append({\n",
    "            \"POLICY NAME\": policy_of_interest,\n",
    "            \"Y/N/M\": \"ERROR\",\n",
    "            \"POLICY DETAILS\": f\"Failed to parse: {str(e)}\"\n",
    "        })\n",
    "        status = \"Parse Error\"\n",
    "\n",
    "    print_progress_bar(i + 1, len(policy_of_interests), \n",
    "                       prefix=f\"{n_count } N's | {policy_of_interest} → {status}\",\n",
    "                       start_time=start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reformat the response into a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(parsed_results)\n",
    "results_df.rename(columns={\"Status\": \"Y/N/M\"}, inplace=True)\n",
    "results_df = results_df[[\"Y/N/M\", \"POLICY NAME\", \"POLICY DETAILS\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(f\"./extracted_policy_details/{row_range}_top_{find_top_k}_{n_count}Ns.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y/N/M</th>\n",
       "      <th>POLICY NAME</th>\n",
       "      <th>POLICY DETAILS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Y</td>\n",
       "      <td>BFS (CMHC Program)</td>\n",
       "      <td>The document explicitly states that Tarion war...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Y</td>\n",
       "      <td>BFS ALT-A</td>\n",
       "      <td>The document explicitly mentions 'Tarion warra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Y</td>\n",
       "      <td>BFS Stated Income (Bank Statements)</td>\n",
       "      <td>The lender will conduct a credit check to asse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Y</td>\n",
       "      <td>BFS Stated Income (Conventional)</td>\n",
       "      <td>Tarion warranty required</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Y</td>\n",
       "      <td>BFS Stated Income (Sagen &amp; CG Program)</td>\n",
       "      <td>The maximum number of properties a client can ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Y</td>\n",
       "      <td>Cash Back Mortgages</td>\n",
       "      <td>Maximum number of properties a client can have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Y</td>\n",
       "      <td>Collateral Switch/Transfer</td>\n",
       "      <td>The document explicitly states 'consent to rel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Y</td>\n",
       "      <td>Construction</td>\n",
       "      <td>The use of DocuSign for electronic signatures ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Y</td>\n",
       "      <td>Cottage/Recreational Properties</td>\n",
       "      <td>Tarion warranty required for new homes. Tarion...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Y</td>\n",
       "      <td>Equity Program</td>\n",
       "      <td>The document explicitly mentions Tarion warran...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Y/N/M                             POLICY NAME  \\\n",
       "0     Y                      BFS (CMHC Program)   \n",
       "1     Y                               BFS ALT-A   \n",
       "2     Y     BFS Stated Income (Bank Statements)   \n",
       "3     Y        BFS Stated Income (Conventional)   \n",
       "4     Y  BFS Stated Income (Sagen & CG Program)   \n",
       "5     Y                     Cash Back Mortgages   \n",
       "6     Y              Collateral Switch/Transfer   \n",
       "7     Y                            Construction   \n",
       "8     Y         Cottage/Recreational Properties   \n",
       "9     Y                          Equity Program   \n",
       "\n",
       "                                      POLICY DETAILS  \n",
       "0  The document explicitly states that Tarion war...  \n",
       "1  The document explicitly mentions 'Tarion warra...  \n",
       "2  The lender will conduct a credit check to asse...  \n",
       "3                           Tarion warranty required  \n",
       "4  The maximum number of properties a client can ...  \n",
       "5  Maximum number of properties a client can have...  \n",
       "6  The document explicitly states 'consent to rel...  \n",
       "7  The use of DocuSign for electronic signatures ...  \n",
       "8  Tarion warranty required for new homes. Tarion...  \n",
       "9  The document explicitly mentions Tarion warran...  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
