{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Imports & Setup`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### External Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import mammoth\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import json\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = device.type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### >>> Config Variables <<<"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Row Range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `full`\n",
    "\n",
    "- `first_half`\n",
    "\n",
    "- `second_half`\n",
    "\n",
    "- `random_X`: replace X with the number of policies to be randomly selected\n",
    "\n",
    "- `the_name_of_the_policy`: replace with the name of the policy\n",
    "\n",
    "- `from_X_to_Y` / `from_start_to_Y` / `from_X_to_end` : replace X and Y with the range of policies to be selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_range = \"from_240_to_end\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find Top K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`find_top_k` number of paragraphs that have the highest similarity score with the policy and search terms will be used to generate the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_top_k = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Policy Weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`score_of_similarity`\n",
    "\n",
    "$ =\\ $`policy_weight` $\\times$ `cosine_similarity` between policy and paragraph $ + ( 1 - $`policy_weight` $) \\times$ `cosine_similarity` between policy and search terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_weight = 0.65"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extracting Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`extracting_model` will be used to extract the policy details by using the policy name and the search terms.\n",
    "\n",
    "- `deepseek-r1:8b`\n",
    "\n",
    "- `deepseek-r1:14b`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracting_model = \"deepseek-r1:8b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Embedding Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`embedding_model` will be used to generate the embeddings for the policy and the paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = {\"gpu\": \"intfloat/e5-base\", \"cpu\": \"nomic-embed-text\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prompt_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`prompt_template` is the template that will be used to generate the prompt for the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "    You are an expert in policy analysis. Your task is to extract **form-related policies** from the document titled *Policies_Meridian.docx*.\n",
    "    \n",
    "    1. **Explicitly stated** — The policy is clearly and directly described in the text, with no ambiguity or need for interpretation.\n",
    "    2. **Implicitly referenced** — The policy is not stated directly, but its presence can be **reasonably and confidently inferred** from specific procedures, requirements, or descriptions that **clearly align with the policy's core intent**.\n",
    "\n",
    "    ❗ Do **not** infer based on vague, generic, or loosely related content.  \n",
    "    ❗ If the policy is not clearly present or the reference is too indirect or speculative, treat it as not included.\n",
    "\n",
    "    ---\n",
    "    \n",
    "    ### Document Content:\n",
    "    Below is a selection of paragraphs extracted from *Policies_Meridian.docx*:\n",
    "    \n",
    "    ${combined_paragraphs}$\n",
    "    \n",
    "    ---\n",
    "\n",
    "    ### Target Policy:\n",
    "    - Policy of Interest: **\"${policy_of_interest}$\"**\n",
    "    - Search Term Synonyms: **\"${search_terms_synonym}$\"**\n",
    "    \n",
    "    Use these keywords and any related concepts to locate relevant policies. \n",
    "    Remember to extract not just explicit mentions but also policies that are implied or embedded in procedures.\n",
    "    Do **not** guess or make assumptions. Only mark a policy as found if there is **clear textual evidence**.\n",
    "    \n",
    "    ---\n",
    "    \n",
    "    ### Output Instructions:\n",
    "    \n",
    "    For **each policy instance** found, provide the following:\n",
    "\n",
    "    1. **Y/N/M**:\n",
    "    - `\"Y\"` - Clearly mentioned (explicitly and unambiguously stated).\n",
    "    - `\"M\"` - Mentioned indirectly (implied, inferred, or part of a procedure).\n",
    "    - `\"N\"` - The policy does not appear in the document in any clear or inferable form.\n",
    "    ⛔ Do not guess. If unsure, default to `\"N\"`.\n",
    "\n",
    "    2. **POLICY DETAILS**:\n",
    "    - Copy the **exact original sentence(s)** that describe or imply the policy. If it’s implied, use only specific and logically tied text — no rewording. \n",
    "    - This is a highly sensitive policy detail extraction. **Do not paraphrase. Use the original sentence(s) from the document as much as possible**. \n",
    "    - If multiple sentences support the policy, return a **verbatim combination** of those sentences.\n",
    "\n",
    "    ---\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_progress_bar(iteration, total, prefix='', length=40, start_time=None, line_width=256):\n",
    "    elapsed = time.time() - start_time if start_time else 0\n",
    "    avg_time = elapsed / iteration if iteration > 0 else 0\n",
    "    eta = avg_time * (total - iteration)\n",
    "\n",
    "    percent = f\"{100 * (iteration / float(total)):.1f}\"\n",
    "    filled_length = int(length * iteration // total)\n",
    "    bar_color = '\\033[31m'\n",
    "    bar = bar_color + '█' * filled_length + '-' * (length - filled_length) + '\\033[0m'\n",
    "\n",
    "    eta_min = int(eta // 60)\n",
    "    eta_sec = int(eta % 60)\n",
    "\n",
    "    line = f\"|{bar}| {percent}% Complete | ETA: {eta_min}m {eta_sec}s | {prefix}\"\n",
    "    padded_line = line.ljust(line_width)\n",
    "\n",
    "    sys.stdout.write('\\r' + padded_line)\n",
    "    sys.stdout.flush()\n",
    "def load_docx_to_markdown(docx_path):\n",
    "    with open(docx_path, \"rb\") as docx_file:\n",
    "        result = mammoth.convert_to_markdown(docx_file)\n",
    "        markdown = result.value\n",
    "    return markdown\n",
    "def split_text_into_paragraphs(text, chunk_size=3, merge_headings=True):\n",
    "    \"\"\"Split policy text into N-sentence chunks.\"\"\"\n",
    "\n",
    "    # Fix encoding and glued terms (e.g. BenchmarkRate → Benchmark Rate)\n",
    "    text = text.replace(\"�\", \" \").replace(\"•\", \"*\")\n",
    "    text = re.sub(r'(?<=[a-z])(?=[A-Z])', ' ', text)        # aB → a B\n",
    "    text = re.sub(r'(?<=\\d)(?=[A-Z])', ' ', text)           # 25Years → 25 Years\n",
    "    text = re.sub(r'(?<=[a-zA-Z])(?=\\d)', ' ', text)        # abc123 → abc 123\n",
    "    text = re.sub(r'(?<=[a-z])(?=[A-Z][a-z])', '. ', text)  # add inferred periods\n",
    "\n",
    "    # Normalize spacing\n",
    "    text = re.sub(r'\\n{2,}', '\\n', text)      # collapse double line breaks\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # remove excess whitespace\n",
    "\n",
    "    # Split at likely section headings\n",
    "    sections = re.split(r'\\n(?=[A-Z][^\\n]{3,60}\\n)', text)\n",
    "\n",
    "    tokenizer = PunktSentenceTokenizer()\n",
    "    chunks = []\n",
    "\n",
    "    for section in sections:\n",
    "        section = section.strip()\n",
    "        if not section:\n",
    "            continue\n",
    "\n",
    "        # Convert bullet-style lines into full sentences\n",
    "        section = re.sub(r\"\\n\\s*\\*\\s*\", \". \", section)\n",
    "        section = re.sub(r\"\\*\\s*\", \"\", section)\n",
    "\n",
    "        # Break before heading-like phrases\n",
    "        section = re.sub(r'(?<=\\. )([A-Z][^\\n]{3,60})(?= )', r'\\n\\1', section)\n",
    "\n",
    "        sentences = tokenizer.tokenize(section)\n",
    "\n",
    "        # Attach short heading-only lines to previous chunk\n",
    "        if merge_headings and len(sentences) <= 1 and chunks:\n",
    "            chunks[-1] += \" \" + section\n",
    "            continue\n",
    "\n",
    "        # Group into N-sentence chunks\n",
    "        for i in range(0, len(sentences), chunk_size):\n",
    "            chunk = \" \".join(sentences[i:i + chunk_size])\n",
    "            chunks.append(chunk.strip())\n",
    "\n",
    "    return chunks\n",
    "def get_embedding(text):\n",
    "    if device == \"cuda\":\n",
    "        tokenizer = AutoTokenizer.from_pretrained(embedding_model[\"gpu\"])\n",
    "        model = AutoModel.from_pretrained(embedding_model[\"gpu\"]).to(\"cuda\")\n",
    "        \n",
    "        if isinstance(text, str):\n",
    "            text = [text]\n",
    "        \n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        return embeddings.cpu().detach().numpy().tolist()[0]\n",
    "    else:\n",
    "        model = embedding_model[\"cpu\"]\n",
    "        \n",
    "        url = \"http://localhost:11434/api/embed\"\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"input\": text\n",
    "        }\n",
    "        response = requests.post(url, json=payload)\n",
    "        return response.json()[\"embeddings\"][0]\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    vec1, vec2 = np.array(vec1), np.array(vec2)\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "def generate_prompt(policy_of_interest, search_terms_synonym, policy_of_interest_embedding, paragraph_embeddings, search_terms_synonyms_embedding):\n",
    "    most_related_paragraphs = []\n",
    "\n",
    "    for index in range(len(paragraph_embeddings)):\n",
    "        score = policy_weight * cosine_similarity(policy_of_interest_embedding, paragraph_embeddings[index]) + (1 - policy_weight) * cosine_similarity(search_terms_synonyms_embedding, paragraph_embeddings[index])\n",
    "        most_related_paragraphs.append((paragraphs[index], score))\n",
    "        \n",
    "    most_related_paragraphs.sort(key=lambda x: x[1], reverse=True)\n",
    "    most_related_paragraphs = most_related_paragraphs[:find_top_k]\n",
    "    combined_paragraphs = \"\\n\".join([p[0] for p in most_related_paragraphs])\n",
    "    \n",
    "    prompt= prompt_template.replace(\"${combined_paragraphs}$\", combined_paragraphs)\n",
    "    prompt = prompt.replace(\"${policy_of_interest}$\", policy_of_interest)\n",
    "    prompt = prompt.replace(\"${search_terms_synonym}$\", search_terms_synonym)\n",
    "    prompt.strip()\n",
    "\n",
    "    return prompt, combined_paragraphs\n",
    "def request_extracted_policy_detail_from_ollama(prompt, policy_of_interest):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    data = {\n",
    "        \"model\": extracting_model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"format\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"Y/N/M\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"Y\", \"M\", \"N\"]\n",
    "                },\n",
    "                \"POLICY DETAILS\": {\n",
    "                    \"type\": \"string\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"Y/N/M\", \"POLICY DETAILS\"]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    response = response.json()\n",
    "    response_data = response['response']\n",
    "    response_data = json.loads(response_data)\n",
    "    response_data[\"POLICY NAME\"] = policy_of_interest\n",
    "    response_data = json.dumps(response_data)\n",
    "    return response_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Load & Preprocess`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load DOCX and embedding them into vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|\u001b[31m████████████████████████████████████████\u001b[0m| 100.0% Complete | ETA: 0m 0s | Embeddings                                   "
     ]
    }
   ],
   "source": [
    "policies_meridian_markdown = load_docx_to_markdown(\"data/preprocessed_data/policies_meridian.docx\")\n",
    "paragraphs = split_text_into_paragraphs(policies_meridian_markdown)\n",
    "paragraph_embeddings = []\n",
    "      \n",
    "start_time = time.time()\n",
    "for p in paragraphs:\n",
    "   paragraph_embeddings.append(get_embedding(p))\n",
    "   print_progress_bar(len(paragraph_embeddings), len(paragraphs), prefix=\"Embeddings\", start_time=start_time, line_width=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Policies & Search Terms Synonyms and embedding them into vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|\u001b[31m████████████████████████████████████████\u001b[0m| 100.0% Complete | ETA: 0m 0s | Embeddings                                   "
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/preprocessed_data/policies_to_update.csv\")\n",
    "df = df.drop(index=[0,1])\n",
    "df = df.drop(columns=[\"Y/N/M\", \"POLICY DETAILS\", \"Prompt\"])\n",
    "if row_range == \"full\":\n",
    "    df = df.iloc[0:len(df)]\n",
    "elif row_range == \"first_half\":\n",
    "    df = df.iloc[0:len(df)//2]\n",
    "elif row_range == \"second_half\":\n",
    "    df = df.iloc[len(df)//2:len(df)]\n",
    "elif \"random\" in row_range:\n",
    "    row_range = int(row_range.split(\"_\")[1])\n",
    "    df = df.sample(n=row_range, random_state=1)\n",
    "elif \"from_\" in row_range and \"to_\" in row_range:\n",
    "    if \"start\" in row_range:\n",
    "        start_row = 0\n",
    "    else:\n",
    "        start_row = int(row_range.split(\"_\")[1])   \n",
    "    if \"end\" in row_range:\n",
    "        end_row = len(df)\n",
    "    else:\n",
    "        end_row = int(row_range.split(\"_\")[3])   \n",
    "            \n",
    "    if start_row < 0:\n",
    "        start_row = 0\n",
    "    if end_row > len(df):\n",
    "        end_row = len(df)\n",
    "    if start_row > end_row:\n",
    "        print(f\"Invalid range: {start_row} to {end_row}.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    df = df.iloc[start_row:end_row]    \n",
    "else:\n",
    "    df = df[df[\"POLICY NAME\"] == row_range]\n",
    "    if len(df) == 0:\n",
    "        print(f\"Policy {row_range} not found in the CSV file.\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "\n",
    " \n",
    "policy_of_interests = df[\"POLICY NAME\"].tolist()\n",
    "search_terms_synonyms = df[\"Search Terms Synonyms\"].tolist()\n",
    "policy_of_interest_embeddings = []\n",
    "search_terms_synonyms_embeddings = []\n",
    "\n",
    "start_time = time.time()\n",
    "for p in policy_of_interests:\n",
    "    policy_of_interest_embeddings.append(get_embedding(p))\n",
    "    print_progress_bar(len(policy_of_interest_embeddings), len(policy_of_interests), prefix=\"Embeddings\", start_time=start_time, line_width=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|\u001b[31m████████████████████████████████████████\u001b[0m| 100.0% Complete | ETA: 0m 0s | Embeddings                                   "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for s in search_terms_synonyms:\n",
    "    search_terms_synonyms_embeddings.append(get_embedding(str(s)))\n",
    "    print_progress_bar(len(search_terms_synonyms_embeddings), len(search_terms_synonyms), prefix=\"Embeddings\", start_time=start_time, line_width=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Generate Table`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loop through each policy of interest and generate the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|\u001b[31m████████████████████████████████████████\u001b[0m| 100.0% Complete | ETA: 0m 0s | 2 N's | NICHES → N                                                                                                                                                           "
     ]
    }
   ],
   "source": [
    "results = []\n",
    "prompts = []\n",
    "parsed_results = []\n",
    "n_count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "policy_to_paragraphs = []\n",
    "\n",
    "for i, policy_of_interest in enumerate(policy_of_interests):\n",
    "    search_terms_synonym = str(search_terms_synonyms[i])\n",
    "    policy_of_interest_embedding = policy_of_interest_embeddings[i]\n",
    "    search_terms_synonyms_embedding = search_terms_synonyms_embeddings[i]\n",
    "    prompt, combined_paragraphs = generate_prompt(policy_of_interest, search_terms_synonym, policy_of_interest_embedding, paragraph_embeddings, search_terms_synonyms_embedding)\n",
    "    prompts.append(prompt)\n",
    "    policy_to_paragraphs.append({\n",
    "        \"POLICY NAME\": policy_of_interest,\n",
    "        \"Paragraphs\": combined_paragraphs\n",
    "    })\n",
    "\n",
    "for i, policy_of_interest in enumerate(policy_of_interests):\n",
    "    extracted_policy_detail = request_extracted_policy_detail_from_ollama(prompts[i], policy_of_interest)\n",
    "    results.append(extracted_policy_detail)\n",
    "\n",
    "    try:\n",
    "        detail_dict = json.loads(extracted_policy_detail)\n",
    "        parsed_results.append(detail_dict)\n",
    "        \n",
    "        if detail_dict.get(\"Y/N/M\", \"\").strip() == \"N\":\n",
    "            n_count += 1\n",
    "            status = \"N\"\n",
    "        elif detail_dict.get(\"Y/N/M\", \"\").strip() == \"Y\":\n",
    "            status = \"Y\"\n",
    "        elif detail_dict.get(\"Y/N/M\", \"\").strip() == \"M\":\n",
    "            status = \"M\"\n",
    "        else:\n",
    "            status = \"ERROR\"\n",
    "\n",
    "    except Exception as e:\n",
    "        parsed_results.append({\n",
    "            \"POLICY NAME\": policy_of_interest,\n",
    "            \"Y/N/M\": \"ERROR\",\n",
    "            \"POLICY DETAILS\": f\"Failed to parse: {str(e)}\"\n",
    "        })\n",
    "        status = \"Parse Error\"\n",
    "\n",
    "    print_progress_bar(i + 1, len(policy_of_interests), \n",
    "                       prefix=f\"{n_count } N's | {policy_of_interest} → {status}\",\n",
    "                       start_time=start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reformat the response into a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(parsed_results)\n",
    "results_df = results_df[[\"Y/N/M\", \"POLICY NAME\", \"POLICY DETAILS\"]]\n",
    "\n",
    "policy_to_paragraphs_df = pd.DataFrame(policy_to_paragraphs)\n",
    "policy_to_paragraphs_df = policy_to_paragraphs_df[[\"POLICY NAME\", \"Paragraphs\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_date_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "os.makedirs(current_date_time, exist_ok=True)\n",
    "results_df.to_csv(f\"./{current_date_time}/extracted_policy_details.csv\", index=False)\n",
    "policy_to_paragraphs_df.to_csv(f\"./{current_date_time}/policy_to_paragraphs.csv\", index=False)\n",
    "with open(f\"./{current_date_time}/config_variables.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"## Configuration Variables\\n\")\n",
    "    f.write(\"| variable name | value |\\n\")\n",
    "    f.write(\"|---|---|\\n\")\n",
    "    f.write(f\"| device | `{torch.cuda.get_device_name(0) if device == 'cuda' else 'cpu'}` | \\n\")\n",
    "    f.write(f\"| find_top_k | `{find_top_k}` | \\n\")\n",
    "    f.write(f\"| policy_weight | `{policy_weight}` | \\n\")\n",
    "    if device == \"cuda\":\n",
    "        f.write(f\"| embedding_model | `{embedding_model['gpu']}` | \\n\")\n",
    "    else:\n",
    "        f.write(f\"| embedding_model | `{embedding_model['cpu']}` | \\n\")\n",
    "    f.write(f\"| extracting_model | `{extracting_model}` | \\n\")\n",
    "    f.write(f\" \\n **prompt_template**: \\n ```{prompt_template}``` \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
